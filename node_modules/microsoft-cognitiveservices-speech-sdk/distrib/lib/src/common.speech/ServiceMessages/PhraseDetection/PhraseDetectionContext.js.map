{"version":3,"sources":["src/common.speech/ServiceMessages/PhraseDetection/PhraseDetectionContext.ts"],"names":[],"mappings":";AAAA,EAAE;AACF,gDAAgD;AAChD,wGAAwG;AACxG,EAAE;;;AAYF;;GAEG;AACH,IAAY,eAKX;AALD,WAAY,eAAe;IACvB,8CAA2B,CAAA;IAC3B,0CAAuB,CAAA;IACvB,gDAA6B,CAAA;IAC7B,gCAAa,CAAA;AACjB,CAAC,EALW,eAAe,GAAf,uBAAe,KAAf,uBAAe,QAK1B","file":"PhraseDetectionContext.js","sourcesContent":["//\r\n// Copyright (c) Microsoft. All rights reserved.\r\n// Licensed under the MIT license. See LICENSE.md file in the project root for full license information.\r\n//\r\n\r\nimport { Dictation } from \"./Dictation\";\r\nimport { Enrichment } from \"./Enrichment\";\r\nimport { Interactive } from \"./Interactive\";\r\nimport { Conversation } from \"./Conversation\";\r\nimport { SpeakerDiarization } from \"./SpeakerDiarization\";\r\nimport { SentimentAnalysis } from \"./SentimentAnalysis\";\r\nimport { GeoLocation } from \"./GeoLocation\";\r\nimport { OnSuccess } from \"./OnSuccess\";\r\nimport { OnInterim } from \"./OnInterim\";\r\n\r\n/**\r\n * The Recognition modes\r\n */\r\nexport enum RecognitionMode {\r\n    Interactive = \"Interactive\",\r\n    Dictation = \"Dictation\",\r\n    Conversation = \"Conversation\",\r\n    None = \"None\"\r\n}\r\n\r\n/**\r\n * Defines the phrase detection payload in the speech Context message\r\n */\r\nexport interface PhraseDetectionContext {\r\n    /**\r\n     * The initial silence timeout.\r\n     */\r\n    initialSilenceTimeout?: number;\r\n\r\n    /**\r\n     * The trailing silence timeout.\r\n     */\r\n    trailingSilenceTimeout?: number;\r\n\r\n    /**\r\n     * The recognition mode.\r\n     */\r\n    mode?: RecognitionMode;\r\n\r\n    /**\r\n     * The enrichment option.\r\n     */\r\n    enrichment?: Enrichment;\r\n\r\n    /**\r\n     * The Interactive options.\r\n     */\r\n    interactive?: Interactive;\r\n\r\n    /**\r\n     * The Dictation options.\r\n     */\r\n    dictation?: Dictation;\r\n\r\n    /**\r\n     * The Conversation options.\r\n     */\r\n    conversation?: Conversation;\r\n\r\n    /**\r\n     * The grammar scenario that allows clients to use sophisticated acoustic and language models\r\n     */\r\n    grammarScenario?: string;\r\n\r\n    /**\r\n     * A flag that indicates whether to enable interim results or not. If true, interim results are returned to the client application.\r\n     */\r\n    interimResults?: boolean;\r\n\r\n    /**\r\n     * The configuration of speaker diarization.\r\n     */\r\n    speakerDiarization?: SpeakerDiarization;\r\n\r\n    /**\r\n     * The configuration of sentiment analysis.\r\n     */\r\n    sentimentAnalysis?: SentimentAnalysis;\r\n\r\n    /**\r\n     * The geo location.\r\n     */\r\n    geoLocation?: GeoLocation;\r\n\r\n    /**\r\n     * The on success.\r\n     */\r\n    onSuccess?: OnSuccess;\r\n\r\n    /**\r\n     * The on interim.\r\n     */\r\n    onInterim?: OnInterim;\r\n\r\n    /**\r\n     * The mapping from language to custom model id, if required.\r\n     */\r\n    customModels?: CustomLanguageMappingEntry[];\r\n\r\n    /**\r\n     * The detection language.\r\n     */\r\n    language?: string;\r\n}\r\n\r\n/**\r\n * Defines a mapping entry from a language to a custom endpoint.\r\n */\r\nexport interface CustomLanguageMappingEntry {\r\n    /**\r\n     * The language for there is a custom endpoint.\r\n     */\r\n    language: string;\r\n\r\n    /**\r\n     * The custom endpoint id.\r\n     */\r\n    endpoint: string;\r\n}\r\n"]}